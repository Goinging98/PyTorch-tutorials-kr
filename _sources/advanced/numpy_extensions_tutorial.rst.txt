

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating extensions using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of it’s implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of it’s implementation



.. code-block:: python


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesn’t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**



.. code-block:: python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):

        def forward(self, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        def backward(self, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction()(input)







**Example usage of the created layer:**



.. code-block:: python


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out::

    tensor([[  1.2398,   9.5269,   4.7623,   5.4044,   8.1176],
            [  4.9276,   4.6501,   6.5627,   5.7318,  12.3326],
            [  5.3815,   7.2504,   2.8178,   6.7118,  16.1105],
            [ 12.8063,   6.7665,   2.1601,   9.2115,   6.2457],
            [  4.3929,   9.4095,   5.1524,   1.8037,  11.6011],
            [ 12.8063,   3.0418,   2.8288,  11.8973,   6.2457],
            [  5.3815,   6.1165,   5.0930,   5.8698,  16.1105],
            [  4.9276,   6.8378,   2.2143,  13.4805,  12.3326]])
    tensor([[ 1.0311e+00, -4.0767e-01,  2.1588e+00,  2.8286e-01,  1.0652e+00,
             -4.0396e-01,  6.0155e-01, -1.8405e+00],
            [-5.0624e-01, -4.1904e-01,  1.1342e+00, -1.5119e+00,  1.0010e+00,
             -1.4249e+00,  1.3772e+00, -3.2493e-01],
            [-1.7739e-05,  1.6333e+00, -7.5139e-01,  1.0282e+00, -7.0394e-01,
              1.3903e-01,  5.3815e-01, -3.1772e-01],
            [-3.3827e-01,  2.4690e-01, -5.8646e-01, -1.1857e+00, -7.0632e-01,
             -5.5951e-01,  5.6833e-03,  5.1649e-01],
            [ 1.8027e-01,  4.1403e-01, -4.1056e-01, -1.0268e+00, -4.4384e-01,
             -7.4212e-02, -1.9847e+00, -4.7740e-02],
            [ 7.6982e-01,  2.8549e-02,  1.2375e+00, -2.4743e-01,  7.3899e-01,
              5.3658e-02,  1.0159e+00,  5.0938e-01],
            [-6.3937e-01,  5.0669e-01,  8.5989e-01, -1.7781e+00, -1.8781e+00,
             -6.2670e-01, -1.9021e+00,  1.9821e+00],
            [ 1.0590e+00, -1.1465e+00,  2.2545e-01,  4.0254e-01, -1.0934e+00,
              6.5063e-01,  3.8383e-01,  2.7039e-01]])


Parametrized example
--------------------

This implements a layer with learnable weights.

It implements the Cross-correlation with a learnable kernel.

In deep learning literature, it’s confusingly referred to as
Convolution.

The backward computes the gradients wrt the input and gradients wrt the
filter.

**Implementation:**

*Please Note that the implementation serves as an illustration, and we
did not verify it’s correctness*



.. code-block:: python


    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter):
            input, filter = input.detach(), filter.detach()  # detach so we can cast to NumPy
            result = correlate2d(input.numpy(), filter.detach().numpy(), mode='valid')
            ctx.save_for_backward(input, filter)
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter = ctx.saved_tensors
            grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full')
            grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid')

            return grad_output.new_tensor(grad_input), grad_output.new_tensor(grad_filter)


    class ScipyConv2d(Module):

        def __init__(self, kh, kw):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(kh, kw))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter)







**Example usage:**



.. code-block:: python


    module = ScipyConv2d(3, 3)
    print(list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print(output)
    output.backward(torch.randn(8, 8))
    print(input.grad)




.. rst-class:: sphx-glr-script-out

 Out::

    [Parameter containing:
    tensor([[-0.6055, -0.1807,  1.4850],
            [ 1.2367, -1.1222,  1.0138],
            [-0.7649, -0.9745,  0.0548]])]
    tensor([[ 0.8499, -4.0000,  1.1173,  4.9666, -5.6359,  1.2353,  2.1116,
              2.9964],
            [ 2.2174, -1.4532, -0.9480,  3.5553, -0.3086,  1.1126, -3.9670,
              3.0703],
            [ 0.3864,  0.5911, -0.1857,  3.3120, -3.1958,  0.7715, -6.7327,
             -0.5456],
            [-1.3444, -2.8088,  2.9493, -0.2762, -2.5846,  5.0385,  3.4354,
             -1.5011],
            [ 1.5296, -2.4519,  1.7683,  0.8858, -0.4722, -0.9711,  2.3750,
             -3.0661],
            [ 4.1665, -3.0542, -4.4661,  3.3573, -2.0429, -1.6781, -0.0433,
             -0.8737],
            [ 0.2843,  1.9322, -1.6338,  1.2417, -0.3862, -1.2904,  3.2702,
             -2.2505],
            [ 4.7768,  0.2091, -0.5893, -1.8875,  0.2965,  0.8954,  2.9632,
             -2.3363]])
    tensor([[ 0.0768,  0.7594, -1.6005,  0.8003, -0.2870,  0.0995,  2.2311,
             -3.5721,  2.0929, -0.5494],
            [ 0.6768, -1.5679,  4.4767, -0.0178,  1.0660, -1.8582, -2.5466,
              2.1006, -1.4706,  0.1192],
            [-0.7522,  0.0397, -2.8065, -0.5532,  1.8882,  2.3626,  1.9200,
             -2.8220, -1.0286,  2.5299],
            [-2.3017, -0.4254, -1.9817, -0.0662, -3.9780,  1.9955, -1.1860,
             -0.8415,  0.4666,  2.6808],
            [ 1.5524,  0.9517, -0.0945, -3.8359,  1.9340,  1.2357, -3.3979,
             -3.6818,  0.1119,  0.7277],
            [ 0.2999,  1.8585,  4.0749, -0.7250,  4.0242,  1.4088,  1.0924,
             -2.3371, -0.4255, -0.6388],
            [ 1.6724, -4.9645, -3.4821, -3.6887,  3.4239, -1.8441, -3.9295,
              3.2099,  0.1233,  0.0581],
            [ 2.4515,  6.3360,  4.3051,  0.2536, -1.9739, -1.8480, -0.4660,
             -0.4909, -0.4673,  0.5835],
            [-3.6699, -1.7539, -1.0450,  0.2442,  0.7463, -2.9405, -2.0416,
             -0.6557,  0.0029,  0.2782],
            [-0.0650,  2.0070,  3.7021,  2.7871,  2.2887,  2.1940,  0.8946,
             -0.3837, -0.3078, -0.0167]])


**Total running time of the script:** ( 0 minutes  0.004 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
